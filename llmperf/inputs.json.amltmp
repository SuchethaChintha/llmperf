{
    'llm_api'="openai",
    'model'="llama-3-70b-instruct",
    'test_timeout_s'=600,
    'max_num_completed_requests'=100,
    'mean_input_tokens='100,
    'stddev_input_tokens'=2,
    'mean_output_tokens'=400,
    'stddev_output_tokens'=2,
    'num_concurrent_requests=10,
    'additional_sampling_params='{}',
    'results_dir="result_outputs_April30_llama-3-70b-instruct_100_400_100_10",
}